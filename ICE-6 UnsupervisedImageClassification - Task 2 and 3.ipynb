{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db6fba37-67a5-4527-8787-2fb13769a192",
   "metadata": {},
   "source": [
    "# Task 2: Optimize the code to improve the accuracy using given tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c1c97a5-f4b3-49b9-913e-9949d9ecef55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "175c1af6-50e5-4e9a-af3a-98382f4c57d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train df shape:  (60000, 785) \n",
      " Test df shape:  (10000, 785)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('MNIST/mnist_train.csv')\n",
    "test_df = pd.read_csv('MNIST/mnist_test.csv')\n",
    "\n",
    "print('Train df shape: ', train_df.shape,'\\n', 'Test df shape: ', test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7b24f7-5a00-47a1-aeb7-3ffc5b35de2f",
   "metadata": {},
   "source": [
    "### Data Preprocessing (Normalizaiton & Standardization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9505792e-f3d8-40e6-b688-e78141ff6855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train Type is:  (60000, 784)\n",
      "Y_train Type is:  (60000,)\n",
      "X_test Type is:  (10000, 784)\n",
      "Y_test Type is:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Setting up and Preparing Training and Testing DataFrames: \n",
    "\n",
    "X_train = train_df.T[1:].T\n",
    "y_train = train_df['label']\n",
    "X_test = test_df.T[1:].T\n",
    "y_test = test_df['label']\n",
    "\n",
    "print('X_train Type is: ', X_train.shape)\n",
    "print('Y_train Type is: ', y_train.shape)\n",
    "print('X_test Type is: ', X_test.shape)\n",
    "print('Y_test Type is: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e75814a-dbc5-4d5d-98eb-3115550c9e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_Train Min. Value =  0\n",
      "Y_Train Man. Value =  255\n"
     ]
    }
   ],
   "source": [
    "# Checking the Min & Max values in the training data\n",
    "train_min = X_train.to_numpy().min()\n",
    "train_max = X_train.to_numpy().max()\n",
    "\n",
    "print('X_Train Min. Value = ', train_min)\n",
    "print('Y_Train Man. Value = ', train_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3562fa1f-2c3a-43d2-8cdd-58f082702604",
   "metadata": {},
   "source": [
    "> NORMALIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7d6e2c-54b1-4de1-962a-ca95584ee4b6",
   "metadata": {},
   "source": [
    "Normalization: it is the process of rescaling data points values to fit between a range of 0 to 1. It essentially imporoves on the model feature and provide better accuracy rate. Mathematically, normalization is calculated with two slightly different formulas:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa3dc4d-4052-4939-8e21-e0a6b458a0c7",
   "metadata": {},
   "source": [
    "            Formula (1): Xnormalized = (X- Xminimum) / range of x        ---> where the range of x = Max value - Min Value\n",
    "    \n",
    "            Formula (2): Xnormalized = a + ( ((X - Xminimum) * (b - a)) / range of X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db83a58f-861b-4b29-8038-58c70c90f783",
   "metadata": {},
   "source": [
    "Formula (2) is mainly used when data is needed to be within a custom range of minimum (a) and maximum (b) values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdac912b-5e34-4806-abc7-167c3ae287c9",
   "metadata": {},
   "source": [
    "> STANDARDIZATION:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fa933d-16a2-416b-906b-42d8b441f59f",
   "metadata": {},
   "source": [
    "For the standardization of the data, we use statistical anylsis techniques which provide better understanding of the data. For example, how distant or close are from each other. In other terms, we can calculate and visualize the distribution of the data points on a scatterplot to look for any abnormalities or high variations (to spot outliers) from which an analyist would be able to obtain insightful information that maybe hidden by these outliers. Then perform some mathematical operations to clean up the and normalize the data to be able to conduct further investigation with higher accuracy.\n",
    "\n",
    "Mathematically:\n",
    "There are different methods used for the standardization of data, such as Z-score, feature clipping and log scaling. The Z-score value tells us how, on average, how far or close each data point is from the mean with a given SD. is calculated using the following equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e989d4-645f-4530-b686-fa7b5cc28420",
   "metadata": {},
   "source": [
    "                       Z-Score of X = (X - u) / SD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58bfa9af-26b5-4815-a8d5-c8839a1ffa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion to float\n",
    "X_train = X_train.astype('float32')  \n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Normalization: should return the minimum and maximum values of X_train to be in the range 0 to 1.\n",
    "# -- for that reason, we converted all integer values to float values in the step above.\n",
    "# Here we used Formula (2) for normalization: where the range = 255 - 0 = 255. And X - Xmin will always be 0, \n",
    "#--- That is why we are dividing by the range of 255.\n",
    "\n",
    "X_train = X_train/255.0   \n",
    "X_test = X_test/255.0\n",
    "\n",
    "# Here we are using the Z-score formula for Standardization:  Z-Score of X = (X - u) / SD\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "standardized_X = scaler.transform(X_train)\n",
    "standardized_X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "29dcc352-528e-424a-a78c-3538cb6b67fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standardized_X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b90e0fc2-3657-4b30-98b4-76490fb4f739",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\snt0112\\Anaconda3\\envs\\ST_Python3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1848: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=4\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MiniBatchKMeans(n_clusters=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MiniBatchKMeans</label><div class=\"sk-toggleable__content\"><pre>MiniBatchKMeans(n_clusters=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MiniBatchKMeans(n_clusters=10)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "total_clusters = len(np.unique(y_test))\n",
    "# Initialize the K-Means model\n",
    "kmeans = MiniBatchKMeans(n_clusters = total_clusters)\n",
    "# Fitting the model to training set\n",
    "kmeans.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "351446a6-3a31-4341-b9a1-8f8497c5544d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 2, 0, ..., 8, 9, 5])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0664445d-24d2-41e4-b4d4-ae53b0623683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_info(cluster_labels,y_train):\n",
    "\n",
    "#Associates most probable label with each cluster in KMeans model\n",
    "#returns: dictionary of clusters assigned to each label\n",
    "\n",
    "  # Initializing\n",
    "  reference_labels = {}\n",
    "  # For loop to run through each label of cluster label\n",
    "  for i in range(len(np.unique(kmeans.labels_))):\n",
    "    index = np.where(cluster_labels == i,1,0)\n",
    "    num = np.bincount(y_train[index==1]).argmax()\n",
    "    reference_labels[i] = num\n",
    "  return reference_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0e927842-b533-4f68-8f1f-5541ece5806b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_labels = retrieve_info(kmeans.labels_,y_train)\n",
    "number_labels = np.random.rand(len(kmeans.labels_))\n",
    "for i in range(len(kmeans.labels_)):\n",
    "  number_labels[i] = reference_labels[kmeans.labels_[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5c896a18-b5d7-47eb-99c0-855aae1c660a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 2, 1: 5, 2: 6, 3: 1, 4: 0, 5: 4, 6: 9, 7: 5, 8: 9, 9: 8, 10: 0, 11: 8, 12: 2, 13: 7, 14: 9, 15: 6, 16: 7, 17: 3, 18: 3, 19: 2, 20: 0, 21: 8, 22: 1, 23: 7, 24: 1, 25: 0, 26: 4, 27: 0, 28: 3, 29: 3, 30: 3, 31: 7, 32: 5, 33: 5, 34: 0, 35: 4, 36: 6, 37: 2, 38: 4, 39: 5}\n"
     ]
    }
   ],
   "source": [
    "print(reference_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7a890215-bf7f-4d7e-87be-8e2b5da0cd59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 0 4 1 7 2 1 3 1 4 3 1 8 6 1 4 2 4 6 4]\n",
      "0     5\n",
      "1     0\n",
      "2     4\n",
      "3     1\n",
      "4     9\n",
      "5     2\n",
      "6     1\n",
      "7     3\n",
      "8     1\n",
      "9     4\n",
      "10    3\n",
      "11    5\n",
      "12    3\n",
      "13    6\n",
      "14    1\n",
      "15    7\n",
      "16    2\n",
      "17    8\n",
      "18    6\n",
      "19    9\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Comparing Predicted values and Actual values\n",
    "print(number_labels[:20].astype('int'))\n",
    "print(y_train[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a226f0ff-6c85-4fc8-9120-d9f2b1c25e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5557166666666666\n"
     ]
    }
   ],
   "source": [
    "# Calculating accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(number_labels,y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6036c82f-8e97-4421-8d35-4ad511df8062",
   "metadata": {},
   "source": [
    "As you can see that the accuracy rate has gone up from 11.3% to 55.6%. That is about 20% improvment. That is actually big jump. And the better improvement performace of our algorithm and imporovement of the accuracy rate was due to the standardization (Z-score) step that we introduced. \n",
    "\n",
    "But, how does the Z-score actually help?\n",
    "\n",
    "The objective of K-meas clustering is that after the initialization step (randomly selecting the first centriods to start the classification process), the K-means algorithm re-checks and re-evaluates the location of all the data points in each cluster, it calculates the mean and assignes a the new centroid for that cluster. It does that for each of the 10 clusters. By calculating the mean, it learns how the data points are distributed, thus making it easier to find update the centroids. Then it repeats the same process iteratively by re-calculating the new distances between every data point in the dataset and the new centroids. Basiclly, it's checking to see if any of the points has come closer to or gone farther from the new centriod. The ultimate goal for the algorithm is not only to group points into clusters based on their distances from the centroid but it also makes sure to do so while reducing the distance to the minimum. Well, here is where the Z-score comes handy! The essense of calculating the Z-score is becuase it tells us how the datas points are ditributed around and how far or close they are from the mean. And this is very important information for the algorithm as it may speed up the clustering and classification process and most importantly improve the predictions accuracy rate based on standardized calculated measures. \n",
    "\n",
    "Also, it's worth mentioning that the Z-score results in a special normal distribution of the data points where the mean is equal to 0 and the standard divation is 1. This allows for easier calculation of the probability of certain values occurring in the distribution, it also makes it eaiser to compare data sets with other clusters of different means and standard deviations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87d0966-b215-4d27-992c-7e668817480a",
   "metadata": {},
   "source": [
    "## But can we do even better than 55% accuracy?? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc1c5c7-bb96-445a-9030-486b4eaff947",
   "metadata": {},
   "source": [
    "The other optimization technique that we will explore is to manipulate the number of clusters. \n",
    "The hypothesis is that, given every other element of the code is properly programed, with more clusters, the easier it is for the algorithm to achieve its goal of reducing the distance between the data points and the centroids. Hence, also reducing the variablity. There, we predict to see better perfomance and improvement in the accuracy rate as we increase the number of clusters.\n",
    "\n",
    "So, we are just going to run the code again and make a simple adjustment by increasing the number of cluster to 40 (randomly selected). Let's how we do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "65d3bcc2-7acd-4cc0-9519-b2d81022d494",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\snt0112\\Anaconda3\\envs\\ST_Python3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1848: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=4\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kmeans Lables:  [ 7 27 26 ...  7 15 35]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "total_clusters = len(np.unique(y_test))\n",
    "# Initialize the K-Means model\n",
    "kmeans = MiniBatchKMeans(n_clusters = 40)\n",
    "# Fitting the model to training set\n",
    "kmeans.fit(X_train)\n",
    "\n",
    "print('\\nKmeans Lables: ', kmeans.labels_)\n",
    "\n",
    "def retrieve_info(cluster_labels,y_train):\n",
    "\n",
    "#Associates most probable label with each cluster in KMeans model\n",
    "#returns: dictionary of clusters assigned to each label\n",
    "\n",
    "  # Initializing\n",
    "  reference_labels = {}\n",
    "  # For loop to run through each label of cluster label\n",
    "  for i in range(len(np.unique(kmeans.labels_))):\n",
    "    index = np.where(cluster_labels == i,1,0)\n",
    "    num = np.bincount(y_train[index==1]).argmax()\n",
    "    reference_labels[i] = num\n",
    "  return reference_labels\n",
    "\n",
    "reference_labels = retrieve_info(kmeans.labels_,y_train)\n",
    "number_labels = np.random.rand(len(kmeans.labels_))\n",
    "for i in range(len(kmeans.labels_)):\n",
    "  number_labels[i] = reference_labels[kmeans.labels_[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8b99070f-6382-4b70-ac3a-330f2767e6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 2, 1: 5, 2: 6, 3: 1, 4: 0, 5: 4, 6: 9, 7: 5, 8: 9, 9: 8, 10: 0, 11: 8, 12: 2, 13: 7, 14: 9, 15: 6, 16: 7, 17: 3, 18: 3, 19: 2, 20: 0, 21: 8, 22: 1, 23: 7, 24: 1, 25: 0, 26: 4, 27: 0, 28: 3, 29: 3, 30: 3, 31: 7, 32: 5, 33: 5, 34: 0, 35: 4, 36: 6, 37: 2, 38: 4, 39: 5}\n"
     ]
    }
   ],
   "source": [
    "print(reference_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3dcfc12d-ab5f-48e6-ab5d-e259bf49b2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 4 2 1 3 1 4]\n",
      "0    5\n",
      "1    0\n",
      "2    4\n",
      "3    1\n",
      "4    9\n",
      "5    2\n",
      "6    1\n",
      "7    3\n",
      "8    1\n",
      "9    4\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Comparing Predicted values and Actual values\n",
    "print(number_labels[:10].astype('int'))\n",
    "print(y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e9bf6874-85e5-4af2-984f-19ad9633882c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7841166666666667\n"
     ]
    }
   ],
   "source": [
    "# Calculating accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(number_labels,y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c4306d-8bcd-4c7f-9045-531e628b763e",
   "metadata": {},
   "source": [
    "And as we can see, the accuracy rate has gone up another 20%. Which means our hypothsis is accepted."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
